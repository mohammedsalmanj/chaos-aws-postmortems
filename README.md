ğŸŒ€ chaos-aws-postmortems

Documenting real-world learning, insights, and case studies in Chaos Engineering â€” with a special focus on AWS outages, control plane failures, and resilience testing.
This repository turns real incidents into hands-on labs, helping teams build stronger systems through GameDays, observability exercises, and chaos experiments.

ğŸ¯ The goal isnâ€™t to break things for fun â€” itâ€™s to understand how things break,
so we can make them unbreakable.


ğŸ“˜ Purpose

Cloud reliability doesnâ€™t come from theory or tools â€” it comes from practice.
This repo acts as a learning ground for SREs, DevOps, and AIOps engineers to:

Study how AWS outages actually unfold

Simulate them in a safe environment

Practice diagnosis, communication, and recovery

Improve automation, monitoring, and response playbooks

Each case study connects:

A real AWS outage or scenario

An analogy anyone can understand

A chaos experiment you can recreate



We explore AWS outages and fault domains through:

ğŸ§© Case Studies â€” Postmortems of real AWS incidents (DNS, EC2, Aurora, EKS, S3, etc.)

âš™ï¸ Chaos Experiments â€” Fault Injection (via AWS FIS / local tools)

ğŸ§  Resilience Practices â€” Runbooks, alerting improvements, and detection patterns

ğŸ”¬ Observability Layers â€” Using CloudWatch, X-Ray, and New Relic for MELT data (Metrics, Events, Logs, Traces)

ğŸ® GameDays â€” Repeatable, controlled drills that simulate real outage conditions

Each module is built to answer three questions:

What failed?

Why did it spread?

What would we do differently next time?


ğŸ§© Case Study: AWS US-East DNS Outage (October 2025)

How a small synchronization fault between AWSâ€™s internal DNS systems triggered a massive, multi-service disruption â€” and what we can learn from it.

ğŸ§  1. Why DNS Is So Critical

Before diving into what broke, itâ€™s important to understand why DNS is so central.
Every app and AWS service relies on DNS to translate a name into a reachable address.

For example:

dynamodb.us-east-1.amazonaws.com â†’ 10.23.40.18


Your app doesnâ€™t know where DynamoDB lives â€” it just knows this name.
If that translation fails, the app canâ€™t reach its database.
Even if DynamoDB is healthy, to the app it looks dead.

ğŸ’¡ Analogy: DNS is like the phonebook of AWS.
When the phonebook goes blank, everyone still exists â€” but no one can call each other.
Services start retrying, crashing, or spinning in loops because they think the other side vanished.

This is why Route 53, AWSâ€™s DNS service, is more than a public resolver â€”
itâ€™s part of AWSâ€™s internal control plane, powering communication between systems like EC2, DynamoDB, CloudWatch, and IAM.

âš™ï¸ 2. The Hidden Mechanism â€” Planner and Enactor

Inside AWS, DNS updates happen dynamically.
As AWS scales up and deploys services, thousands of DNS records are created, updated, or deleted every minute.

This process is managed by two internal components:

Component	Role	Analogy
Planner	Decides what DNS records need to be changed. It creates a â€œto-doâ€ list of updates for Route 53.	ğŸ§  The manager who writes tasks on a whiteboard.
Enactor	Executes those updates â€” applying changes to Route 53 and verifying them.	ğŸ§° The technician who actually performs the updates.

When both stay synchronized, AWSâ€™s internal DNS stays accurate and stable.
If they drift apart â€” bad updates can spread quickly.

ğŸ’¥ 3. What Went Wrong â€” The Race Condition

During a normal update cycle in October 2025, AWS rolled out a new DNS synchronization routine.

The Planner started generating DNS updates at high frequency to reflect scaling in DynamoDB partitions.

The Enactor fell slightly behind due to throttling in its internal queue.

The Planner assumed those old entries were processed and sent â€œblankâ€ updates for them.

Route 53 applied those empty updates, effectively deleting valid DNS entries for key internal services â€” especially DynamoDB control plane endpoints.

ğŸ’¡ Analogy: Imagine the manager updates the whiteboard too fast, thinking the technician already copied it down.
The technician, confused, erases names assuming theyâ€™re done.
Suddenly, no one knows whoâ€™s assigned to what â€” the board is blank.

ğŸŒŠ 4. The Domino Effect â€” How One Bug Cascaded Across AWS

Once DNS records disappeared, the problem wasnâ€™t isolated â€” it cascaded across many AWS systems:

Step 1: DynamoDB â€œdisappearedâ€

EC2, Lambda, and internal AWS tools couldnâ€™t resolve dynamodb.us-east-1.amazonaws.com.

These services depend on DynamoDB for configuration, state tracking, and health data.

Step 2: Auto Scaling went into a loop

Health checks couldnâ€™t reach DynamoDB.

EC2 instances were marked as unhealthy and terminated.

New instances came up, but faced the same DNS failure â€” creating a loop of useless replacements.

Step 3: Control plane overload

The failed health checks triggered retries, API calls, and internal updates at scale.

These retries flooded internal networks, slowing down IAM, S3, CloudFormation, and CloudWatch APIs.

Even AWSâ€™s status page began lagging â€” classic control plane congestion.

ğŸ’¡ Analogy: It was like a city where the phonebook vanished.
People started driving to each otherâ€™s offices to talk, creating traffic jams.
In AWSâ€™s case â€” those â€œtraffic jamsâ€ were retry storms.

ğŸ“‰ 5. Impact
Category	Impact
Affected Region	us-east-1 (primary), ripple effects globally
Duration	~15 hours until full recovery
Services Hit	Route 53, DynamoDB, EC2, Auto Scaling, IAM, S3
Customer Symptoms	API 503s, failed EC2 scaling, stalled CloudFormation stacks, console login failures
Root Cause	Race condition between DNS Planner and Enactor, leading to deletion of internal Route 53 records

ğŸ’¡ Analogy: Healthy systems looked sick â€” like patients misdiagnosed because the hospitalâ€™s record system lost their names.

ğŸ§° 6. AWSâ€™s Recovery Process

Stopped the Planner automation â€“ to prevent more blank DNS updates.

Manually restored DNS records â€“ re-created missing entries for DynamoDB and other control plane services.

Throttled auto-scaling and health checks â€“ stopped unnecessary EC2 terminations.

Validated DNS zones â€“ ensured consistency between Planner and Enactor records.

Deployed permanent fixes â€“ added handshake validation so Planner and Enactor canâ€™t overwrite valid entries again.

Recovery took time mainly due to DNS propagation â€” even once fixed, cached clients and instances took hours to relearn correct entries.

ğŸ” 7. Observability and Detection
Tool	What It Showed
CloudWatch	Spikes in Route 53 resolution errors and EC2 health check failures
New Relic	Dependency maps showing DynamoDB and Route 53 red (unreachable)
X-Ray	Traces failing at DNS resolution stage
CloudTrail	Surge in Auto Scaling termination events
RUM	Increased frontend latency and user errors (503s)

Observability teams noted that DNS failures initially looked like app-level issues â€” a reminder to always trace issues upstream to dependencies.

ğŸ§  8. Lessons Learned
Area	Lesson	Action
Control Plane Awareness	DNS is not background plumbing â€” itâ€™s a core dependency.	Map DNS dependencies in system design reviews.
Automation Safety	Overactive automation can multiply impact.	Build safety levers (pause, backoff) into automation flows.
Observability	Pure metrics can mislead during cascading outages.	Use correlation (logs, traces, MELT).
Preparedness	Teams need DNS outage drills.	Run quarterly GameDays focused on control plane dependencies.
ğŸ’¬ 9. Simplified Analogy â€” â€œThe City Lost Its Phonebookâ€

Imagine AWS as a big connected city:

Every building (service) has a name: â€œPolice HQ,â€ â€œHospital,â€ â€œPower Station.â€

The cityâ€™s phonebook (Route 53) tells everyone how to reach each other.

One night, during maintenance, the team updating the phonebook accidentally erases half the entries.
Now:

The police canâ€™t call the hospital.

Power stations canâ€™t signal control.

Everyone starts panicking, restarting systems, and creating more confusion.

Eventually, the city freezes â€” not because buildings collapsed, but because communication did.
When engineers manually rebuild the phonebook, everything starts flowing again.

ğŸ”„ 10. Why This Case Matters

This outage showed that:

â€œHealthy but unreachableâ€ is a real failure mode.

Systems donâ€™t need to crash â€” they just need to stop talking.

Resilience isnâ€™t about zero downtime â€” itâ€™s about failing safely and recovering fast.

Understanding these dependencies helps engineers design systems that:

Cache DNS safely,

Failover to secondary resolvers,

Detect dependency issues early,

And avoid automation loops that worsen impact.

