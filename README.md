

ğŸŒ€ chaos-aws-postmortems

Documenting real-world outages, resilience failures, and recovery patterns â€” with a focus on AWS control planes, chaos engineering, and observability.

This repository transforms real AWS incidents into hands-on chaos labs, helping teams strengthen reliability through:

GameDays ğŸ•¹ï¸

Observability exercises ğŸ”¬

Fault injection experiments âš™ï¸

ğŸ¯ The goal isnâ€™t to break things for fun â€” itâ€™s to understand how things break,
so we can make them unbreakable.

ğŸ“˜ Purpose

Cloud reliability doesnâ€™t come from tools â€” it comes from practice.
This repo acts as a training ground for SRE, DevOps, and AIOps engineers to:

âœ… Study how AWS outages actually unfold
âœ… Simulate them safely in local or AWS environments
âœ… Practice diagnosis, communication, and recovery
âœ… Improve monitoring, automation, and runbooks

Each case study links together:

A real-world AWS outage

A relatable analogy ğŸ’¡

A reproducible chaos experiment ğŸ’¥

ğŸ§© Repository Structure
Section	Description
Case Studies	Real AWS incident postmortems (DNS, EC2, Aurora, EKS, S3, etc.)
Chaos Experiments	Fault injection using AWS FIS or local simulators
Resilience Practices	Runbooks, recovery drills, and alerting patterns
Observability Layers	Using CloudWatch, X-Ray, and New Relic for MELT data (Metrics, Events, Logs, Traces)
GameDays	Controlled drills simulating AWS control plane failures

Each module helps answer:

ğŸ” What failed?
ğŸ’¥ Why did it spread?
ğŸ§  What would we do differently next time?

ğŸ§© Case Study: AWS US-East DNS Outage (October 2025)
ğŸ§  1. Why DNS Is So Critical

Every AWS service relies on DNS to translate names like:

dynamodb.us-east-1.amazonaws.com â†’ 10.23.40.18


If DNS fails, apps lose their ability to communicate â€” even if the backend is healthy.

ğŸ’¡ Analogy:
DNS is like the phonebook of AWS.
When the phonebook goes blank, everyone still exists â€” but no one can call each other.
Services start retrying, crashing, or looping endlessly.

âš™ï¸ 2. The Hidden Mechanism â€” Planner and Enactor
Component	Role	Analogy
Planner	Decides which DNS records to change	ğŸ§  The manager writing tasks on a whiteboard
Enactor	Executes and validates the DNS updates	ğŸ§° The technician performing the work

When both stay in sync, AWSâ€™s DNS is accurate.
If they drift apart â€” chaos begins.

ğŸ’¥ 3. What Went Wrong â€” The Race Condition

During a DNS sync update in October 2025:

The Planner generated DNS updates rapidly (due to DynamoDB scaling).

The Enactor lagged behind.

The Planner thought updates were complete and sent blank records.

Route 53 applied those blanks â€” deleting valid DNS entries.

ğŸ’¡ Analogy:
The manager erased the whiteboard too soon â€” the technician hadnâ€™t copied the info yet.
Result: half the names vanished.

ğŸŒŠ 4. The Domino Effect â€” How One Bug Cascaded Across AWS

Step 1: DynamoDB â€œdisappeared.â€
â†’ EC2, Lambda, and internal tools couldnâ€™t resolve DynamoDB endpoints.

Step 2: Auto Scaling panicked.
â†’ Health checks failed â†’ instances terminated â†’ endless loop of broken replacements.

Step 3: Control plane overloaded.
â†’ Retry storms flooded IAM, S3, CloudFormation, CloudWatch, and even AWSâ€™s status page.

ğŸ’¡ Analogy:
The phonebook vanished â€” people started driving to each otherâ€™s offices to talk,
causing a traffic jam of retries.

ğŸ“‰ 5. Impact Summary
Category	Details
Region	us-east-1 (ripple effects globally)
Duration	~15 hours
Affected Services	Route 53, DynamoDB, EC2, Auto Scaling, IAM, S3
Customer Symptoms	503 errors, failed scaling, stuck CloudFormation, login failures
Root Cause	Race condition between Planner & Enactor deleting internal Route 53 records

ğŸ’¡ Analogy: Healthy systems looked sick â€” like patients misdiagnosed because the hospital lost their records.

ğŸ§° 6. AWSâ€™s Recovery Process

ğŸ›‘ Stopped the Planner automation

ğŸ” Restored missing DNS records manually

ğŸ§© Throttled auto-scaling to stop instance churn

ğŸ” Validated DNS zones for consistency

ğŸ§± Deployed handshake validation to prevent overwrite mismatches

â³ Recovery took hours â€” not from fixing the bug, but due to DNS cache propagation delays.

ğŸ” 7. Observability & Detection
Tool	What It Showed
CloudWatch	Spikes in Route 53 errors & EC2 health check failures
New Relic	Dependency maps showing DynamoDB/Route 53 red (unreachable)
X-Ray	Traces failed during DNS resolution
CloudTrail	Surge in termination and retry events
RUM	Increased latency and user 503s

Observation: DNS outages look like app failures until traced upstream.

ğŸ§  8. Lessons Learned
Area	Lesson	Action
Control Plane Awareness	DNS is not background plumbing.	Include DNS mapping in design reviews.
Automation Safety	Overactive automation amplifies failure.	Add pause and backoff logic.
Observability	Metrics alone can mislead during cascading failures.	Correlate logs, traces, and MELT.
Preparedness	Teams must drill DNS outages.	Run quarterly GameDays.
ğŸ’¬ 9. Simplified Analogy â€” â€œThe City Lost Its Phonebookâ€

Imagine AWS as a city:

Buildings = Services (Police HQ, Hospital, Power Station)

Phonebook = Route 53

During maintenance, the phonebook team erases half the entries.
Now no one can call anyone.
People rush around trying to connect manually â€” chaos everywhere.

Once the phonebook is rebuilt, the city starts functioning again.
It wasnâ€™t a power failure â€” it was a communication breakdown.

ğŸ”„ 10. Why This Case Matters

This outage proved that:

ğŸ§© â€œHealthy but unreachableâ€ is a real failure mode.

Systems donâ€™t need to crash â€” they just need to stop talking.

Resilience isnâ€™t about zero downtime â€”
Itâ€™s about failing safely, detecting quickly, and recovering gracefully.

Future designs must:

Cache DNS responses safely

Use secondary resolvers

Detect dependency failures early

Prevent automation from compounding chaos

ğŸ§  In Short

â€œWe donâ€™t just document AWS outages â€” we relive them safely.â€
Each failure is a chance to learn how complex systems behave under stress.

This repo module and related experiments were inspired by the
AWS Resilience Workshop - https://catalog.us-east-1.prod.workshops.aws/workshops/eb89c4d5-7c9a-40e0-b0bc-1cde2df1cb97/en-US/environment/bring-your-own

